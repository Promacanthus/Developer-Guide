---
title: "06 注意事项"
date: 2020-07-15T10:09:32+08:00
draft: true
---

## 资源请求与限制

### CPU

设置 CPU 请求有两种常见**错误**：

- 不设置
- 设置的很低

```yaml
# 不设置
resources: {}

# 设置过低
resources:
  requests:
    cpu: "1m"
```

即使节点的 CPU 没有充分利用，如果设置了不必要的 CPU 限制同样会限制 Pod，这也会导致延迟增加。

### 内存

内存的过量使用一样会带来许多问题。

- 达到 CPU 限制值会导致延迟
- 达到内存限制值，Pod 会被**直接杀死**，这就像是 `OOMkill`（内存不足时会自动杀死进程的机制）

> 如果不想发生这样的事情，就不要过度使用内存，而应该使用 `Guaranteed QoS`，设置内存请求值等于限制值。

```yaml
# Burstable QoS 下的资源设置
resources:
  requests:
    memory: "128Mi"
    cpu: "500m"
  limits:
    memory: "256Mi"
    cpu: 2

# Guaranteed QoS 下的资源设置
resources:
  requests:
    memory: "128Mi"
    cpu: 2
  limits:
    memory: "128Mi"
    cpu: 2
```

在设置资源时，可以使用 `metrics-server` 查看容器当前 CPU 和内存的使用情况。如果它已经在服务器端运行，可以运行以下命令：

```bash
kubectl top pods
kubectl top pods --containers
kubectl top nodes
```

通过当前使用情况，可以大致了解资源情况。

要及时查看情况指标，例如峰值 CPU 使用情况等，可以使用 `Prometheus`、`DataDog` 等。它们会从 `metrics-server` 中获取指标并进行存储，然后可以查询或绘制图形。

`VerticalPodAutoscaler` 工具可以自动化地查看 CPU、内存的使用情况，并根据情况重新设置新的请求值和限制。与之对应的还有[HPA](../05-hpa)。

## liveness和readiness探针

默认情况下，系统不会设置 `liveness` 和 `readiness` 探针。如果设置，那么这两种探针要在 Pod **整个生命周期**中运行。

Kubernetes 强大的自愈能力可以让容器一直工作下去。但是：

- 如果容器内的进程出现不可恢复的错误时，服务要如何重新启动？
- 负载均衡器如何判断 Pod 是否可以开始处理流量，是否可以继续处理更多流量？

### 区别

- 如果对 Pod 的 `liveness` 探测失败，会**重新**启动该 Pod
- 如果对 Pod 的 `readiness` 探测失败，会将 Pod 和 Kubernetes **断开连接**（可以使用 `kubectl get endpoints` 进行检查），并且在下次探测成功之前，都不会发送流量

`readiness` 探测成功告知 Kubernetes 服务 Pod 就绪，可以开始为流量提供服务。

> 在 Pod 的生命周期内，Pod 有没有因为太“热”而无法处理过多的流量，需要减少工作“冷静”一下。直到 `readiness` 探测成功时，再继续给 Pod 发送更多流量。

在这种情况下， `liveness` 探测失败就会适得其反，因为不需要重新启动这个运行状况良好的 Pod。

有时候，不配置任何探针会比配置错误探针要好。

如果将 `liveness` 探针配置成和 `readiness` 探针一样，那么会导致很多问题。

一开始建议仅配置 `readiness` 探针，因为 `liveness` 探针很危险。

如果有一个和其他 Pod 有共享依赖项的 Pod 被关闭，就要保证这个 Pod 的任何一个探针都不能失败，否则将导致所有 Pod 的级联失效。

## HTTP服务的负载均衡器

集群内的（微）服务可以通过 `ClusterIP` 服务和 `DNS Service Discovery` 进行通信。注意不要使用公共 DNS/IP，这会影响延迟并增加云成本。

1. `type: LoadBalancer`：提供并配置一个外部负载均衡器（L7/L4），这些资源（外部静态IPv4，硬件）可能会变得很贵
2. `type: NodePort`：部署一个边缘路由器（Nginx-Ingress-Controller/traefik）作为暴露给外部负载均衡去的单个`NodePort endpoint`，并根据 Kubernetes ingress resource 配置在集群中分配路由流量。

所有流量都在集群内路由到 `NodePort` 服务上，该服务默认 `externalTrafficPolicy: Cluster`，这意味着集群中的每个节点都打开了 `NodePort` ，这样可以使用任何一个与所需的服务（一组 Pod）进行通信。

通常，`NodePort` 服务为针对的 Pod 仅运行在那些节点的子集上。这意味着，如果与未运行 Pod 的节点通信，它会将流量转发到另一个节点，从而导致额外的网络跳转并增加延迟。

在 Kubernetes 服务上设置 `externalTrafficPolicy: Local` 后就不会在每个节点上打开 `NodePort`，只会在实际运行 Pod 的节点上打开。

如果使用外部负载均衡器对其终端节点进行检查，它会仅将流量发送到应该去往的那些节点，可以改善延迟并减少计算开销和出口成本。

## 设置Pod亲和性

应该明确定义Pod的亲和性，这样可以确保将 Pod 调度在不同的节点上（仅在调度期间，而不是在执行期间进行检查，因此要设置 `requiredDuringSchedulingIgnoredDuringExecution`）。

```yaml
# omitted for brevity
  labels:
    app: zk
# omitted for brevity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
          matchExpressions:
            - key: "app"
              operator: In
              values:
              - zk
```

## 设置PodDisruptionBudget

在 Kubernetes 上运行生产工作负载时，节点和集群必须不时地升级或停用。`PodDisruptionBudget`（PDB）是一种 API，为集群管理员和集群用户提供服务保证。

确保创建 PDB 以避免由于节点停用而造成不必要的服务中断。

```yaml
# 保证ZK至少2个
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: zookeepe
```
