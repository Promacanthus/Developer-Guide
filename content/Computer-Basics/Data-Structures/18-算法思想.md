---
title: "18 算法思想"
date: 2020-07-11T15:02:03+08:00
draft: true
---

## 贪心算法

贪心算法有很多经典的应用，比如：

- 霍夫曼编码（Huffman Coding）
- Prim
- Kruskal 最小生成树算法
- Dijkstra 单源最短路径算法

### 例子

1. 假设有一个可以容纳 100kg 物品的背包，可以装各种物品。
2. 有以 5 种豆子，每种豆子的总量和总价值都各不相同。
3. 为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？

物品|总量(KG)|总价值(元)
---|---|---
黄豆|100|100
绿豆|30|90
红豆|60|120
黑豆|20|80
青豆|50|75

1. 先算一算每个物品的单价，按照单价由高到低依次来装
2. 单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆
3. 所以往背包里装 20kg 黑豆、30kg 绿豆、50kg 红豆

这个问题的解决思路本质上就是贪心算法。

### 定义

**第一步，当看到这个问题的时候，首先要联想到贪心算法**：针对一组数据（5种豆子），定义了限制值（100kg）和期望值（总价值），希望从中选出几个数据，在满足限制值的情况下，期望值最大。

**第二步，尝试下这个问题是否可以用贪心算法解决**：每次选择当前情况下，在对限制值同等贡献量（重量相同）的情况下，对期望值（总价值）贡献最大的数据。

**第三步，举几个例子看贪心算法产生的结果是否是最优**。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。

实际上，用贪心算法解决问题的思路，并不总能给出最优解。

在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。

贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。

按照这种思路，求出的最短路径是 `S->A->E->T`，路径长度是 1+4+4=9。

但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径 `S->B->D->T` 才是最短路径，路径的长度是 2+2+2=6。

![image](/images/2de91c0afb0912378c5acf32a173f642.jpg)

> 在这个问题上，贪心算法不工作的主要原因是，**前面的选择，会影响后面的选择**。如果第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。

### 实战

#### 分糖果

> 有 m 个糖果和 n 个孩子，现在要把糖果分给这些孩子吃，但是糖果少，孩子多（`m<n`），所以糖果只能分配给一部分孩子。每个糖果的大小不等，这 m 个糖果的大小分别是 `s1，s2，s3，……，sm`。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这 n 个孩子对糖果大小的需求分别是 `g1，g2，g3，……，gn`。如何分配糖果，能尽可能满足最多数量的孩子？

把这个问题抽象成：

- 一组数据：从 n 个孩子中，抽取一部分孩子分配糖果
- 限制值：糖果个数 m
- 期望值：让满足的孩子的个数是最大的

用贪心算法来解决。

1. 对于一个孩子来说，如果小的糖果可以满足，就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。对糖果大小需求小的孩子更容易被满足，所以从需求小的孩子开始分配糖果。**因为满足一个需求大的孩子跟满足一个需求小的孩子，对期望值的贡献是一样的**。
2. 每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。

#### 钱币找零

> 假设有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 `c1、c2、c5、c10、c20、c50、c100`。现在要用这些钱来支付 K 元，最少要用多少张纸币呢？

把这个问题抽象成：

- 一组数据：若干纸币
- 限制值：K元
- 期望值：纸币数量最少

用贪心算法来解决。**在贡献相同期望值的情况下，希望多贡献金额**。

1. 每次选择小于K元的面额的纸币中面额最大的
2. 选择该面额若干张，保证面额总和小于K
3. 对剩余的值继续上述两个操作

#### 区间覆盖

> 假设有 n 个区间，区间的起始端点和结束端点分别是`[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]`。从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？

把这个问题抽象成：

- 一组数据：n个区间
- 限制值：区间不相交
- 期望值：区间数量最多

用贪心算法来解决。

1. 假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上
2. 按照起始端点从小到大的顺序对这 n 个区间排序
3. 每次选择左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间

### 霍夫曼编码

霍夫曼编码，**利用贪心算法来实现对数据压缩编码，有效节省数据存储空间**。

> 假设有一个包含 1000 个字符的文件，每个字符占 1 个 byte（1byte=8bits），存储这 1000 个字符就一共需要 8000bits，那有没有更加节省空间的存储方式呢？

1. 假设通过统计分析发现，这 1000 个字符中只包含 6 种不同字符，假设它们分别是 a、b、c、d、e、f
2. 而 3 个二进制位（bit）就可以表示 8 个不同的字符，所以，为了尽量减少存储空间，每个字符用 3 个二进制位来表示
3. 存储这 1000 个字符只需要 3000bits 就可以了，比原来的存储方式节省了很多空间

```bash
a(000)、b(001)、c(010)、d(011)、e(100)、f(101)
```

还有更加节省空间的存储方式，霍夫曼编码，一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。

霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的**频率**，根据频率的不同，选择不同长度的编码。

**霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率**。

如何给不同频率的字符选择不同长度的编码呢？

把这个问题抽象成：

- 一组数据：给定的文本
- 限制值：单个字符的长度
- 期望值：存储空间最小

用贪心算法来解决。

- 把出现频率比较多的字符，用稍微短一些的编码
- 把出现频率比较少的字符，用稍微长一些的编码

> 对于**等长**的编码来说，解压缩起来很简单。比如上面例子中`abcdef`用 3 个 bit 表示一个字符。在解压缩的时候，每次从文本中读取 3 位二进制码，然后翻译成对应的字符。

霍夫曼编码是**不等长**的，每次应该读取读取的位数无法确定，这个问题就导致霍夫曼编码解压缩起来比较复杂。**为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况**。

![image](/images/02ad3e02429b294412fb1cff1b3d3829.jpg)

1. 假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。
2. 编码成如下表格，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。

经过这种编码压缩之后，这 1000 个字符只需要 2100bits 就可以了。

字符|出现频率|编码|总二进制位数
---|---|---|---
a|450|1|450
b|350|01|700
c|90|001|270
d|60|0001|150
f|20|00000|100

霍夫曼编码中，如何根据字符出现频率的不同，给不同的字符进行不同长度的编码，这里的处理稍微有些技巧。

1. 把每个字符看作一个节点，并且附带着把频率放到优先级队列中
2. 从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C（**频率小的放在左边**）
3. 把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点
4. 再把 C 节点放入到优先级队列中
5. 重复这个过程，直到队列中没有数据

![image](/images/7b6a08e7df45eac66820b959c64f877a.jpg)

给每一条边加上一个权值：

- 指向左子节点的边统统标记为 0
- 指向右子节点的边，统统标记为 1

从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。

![image](/images/ccf15d048be005924a409574dce143ed.jpg)

实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。

## 分治算法

分治算法（divide and conquer）的核心思想其实就是四个字，**分而治之**，将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，**递归**地解决这些子问题，然后再合并其结果，就得到原问题的解。

**分治算法是一种处理问题的思想，递归是一种编程技巧**。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：

1. 分解：将原问题分解成一系列子问题
2. 解决：递归地求解各个子问题，若子问题足够小，则直接求解
3. 合并：将子问题的结果合并成原问题

分治算法能解决的问题，一般需要满足下面这几个条件：

- 原问题与分解成的小问题具有相同的模式
- 原问题分解成的子问题可以独立求解，**子问题之间没有相关性**（这是分治与动态规划的明显区别）
- 具有分解终止条件，当问题足够小时，可以直接求解
- 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果

### 分治算法应用

使用分治算法解决排序问题。在排序算法中，用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。

假设有 n 个数据，期望数据从小到大排列，那完全有序的数据的有序度就是 `n(n-1)/2`，逆序度等于 0；相反，倒序排列的数据的有序度就是 0，逆序度是 `n(n-1)/2`。除了这两种极端情况外，通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度，如何编程求出一组数据的有序对个数或者逆序对个数。

1. 解法一：将每个数字与后面的数字比较，看有几个比它小，记作k，依次类推，每个数都考察一遍，将k值球和，得到逆序对个数。时间复杂度$$O(n^2)$$。
2. 解法二：分治算法，将数组分为前后两部分（A1,A2），分别求逆序对个数（k1,k2），再计算A1和A2之间的逆序对个数k3，求和`k1+k2+k3`。

**分治算法的一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了**。

解法二中如何快速计算出两个子问题 A1 与 A2 之间的逆序对个数呢？可以使用**归并排序**。归并排序中非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，就可以计算这两个小数组的逆序对个数了。每次合并操作，都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。

![image](/images/e835cab502bec3ebebab92381c667532.jpg)

### 分治算法海量数据应用

分治算法思想的应用是非常广泛的：

- 不仅限于指导编程和算法设计
- 还经常用在海量数据处理的场景中

大部分数据结构和算法都是基于内存存储和单机处理。

但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。

> 比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而单机的内存可能只有 2、3GB，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。

要解决数据量大到内存装不下的问题，利用分治的思想。

1. 将海量的数据集合根据某种方法，划分为几个小的数据集合
2. 每个小的数据集合单独加载到内存来解决
3. 再将小数据集合合并成大数据集合

实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。

上面的例子，先扫描一遍订单，根据订单的金额，将 10GB 的文件划分为几个金额区间。

- 比如订单金额为 1 到 100 元的放到一个小文件
- 101 到 200 之间的放到另一个文件
- 以此类推

每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的 10GB 订单数据了。

> 如果订单数据存储在类似 GFS 这样的分布式系统上，当 10GB 的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多。但是，注意：**数据的存储与计算所在的机器是同一个或者在网络中靠的很近**（比如一个局域网内，数据存取速度很快），否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。

实际上：

- MapReduce 框架只是一个**任务调度器**
- 底层依赖 GFS 来**存储数据**
- 依赖 Borg **管理机器**

它从 GFS 中拿数据，交给 Borg 中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从 Borg 中调度一台机器执行。

- 可以用来处理数据与数据之间**存在关系**的任务，比如 MapReduce 的经典例子，统计文件中单词出现的频率
- 可以用来处理数据与数据之间**没有关系**的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系

## 回溯算法

## 动态规划
